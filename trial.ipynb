{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/aayushjoshi/ML/MLenv/lib/python3.11/site-packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>admiration</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>approval</th>\n",
       "      <th>caring</th>\n",
       "      <th>confusion</th>\n",
       "      <th>curiosity</th>\n",
       "      <th>...</th>\n",
       "      <th>love</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5426</td>\n",
       "      <td>5426</td>\n",
       "      <td>5426</td>\n",
       "      <td>5426</td>\n",
       "      <td>5426</td>\n",
       "      <td>5426</td>\n",
       "      <td>5426</td>\n",
       "      <td>5426</td>\n",
       "      <td>5426</td>\n",
       "      <td>5426</td>\n",
       "      <td>...</td>\n",
       "      <td>5426</td>\n",
       "      <td>5426</td>\n",
       "      <td>5426</td>\n",
       "      <td>5426</td>\n",
       "      <td>5426</td>\n",
       "      <td>5426</td>\n",
       "      <td>5426</td>\n",
       "      <td>5426</td>\n",
       "      <td>5426</td>\n",
       "      <td>5426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>5426</td>\n",
       "      <td>5423</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>edgurhb</td>\n",
       "      <td>I like it!</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4938</td>\n",
       "      <td>5123</td>\n",
       "      <td>5231</td>\n",
       "      <td>5123</td>\n",
       "      <td>5029</td>\n",
       "      <td>5273</td>\n",
       "      <td>5274</td>\n",
       "      <td>5178</td>\n",
       "      <td>...</td>\n",
       "      <td>5174</td>\n",
       "      <td>5405</td>\n",
       "      <td>5217</td>\n",
       "      <td>5411</td>\n",
       "      <td>5299</td>\n",
       "      <td>5408</td>\n",
       "      <td>5358</td>\n",
       "      <td>5283</td>\n",
       "      <td>5297</td>\n",
       "      <td>3660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID        Text admiration amusement  anger annoyance approval  \\\n",
       "count      5426        5426       5426      5426   5426      5426     5426   \n",
       "unique     5426        5423          2         2      2         2        2   \n",
       "top     edgurhb  I like it!      False     False  False     False    False   \n",
       "freq          1           3       4938      5123   5231      5123     5029   \n",
       "\n",
       "       caring confusion curiosity  ...   love nervousness optimism  pride  \\\n",
       "count    5426      5426      5426  ...   5426        5426     5426   5426   \n",
       "unique      2         2         2  ...      2           2        2      2   \n",
       "top     False     False     False  ...  False       False    False  False   \n",
       "freq     5273      5274      5178  ...   5174        5405     5217   5411   \n",
       "\n",
       "       realization relief remorse sadness surprise neutral  \n",
       "count         5426   5426    5426    5426     5426    5426  \n",
       "unique           2      2       2       2        2       2  \n",
       "top          False  False   False   False    False   False  \n",
       "freq          5299   5408    5358    5283     5297    3660  \n",
       "\n",
       "[4 rows x 30 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/aayushjoshi/ML/Projects/emotions/dataset/dataset_val.csv')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring',\n",
       "       'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval',\n",
       "       'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief',\n",
       "       'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization',\n",
       "       'relief', 'remorse', 'sadness', 'surprise', 'neutral'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = df.columns[2:]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_labels = ['joy', 'sadness', 'nervousness', 'anger', 'disgust', 'optimism', 'grief', 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(reduced_labels)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in labels:\n",
    "    if label not in reduced_labels:\n",
    "        df = df.drop(label, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:, 2:] = df.iloc[:, 2:].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Text', 'anger', 'disgust', 'grief', 'joy', 'nervousness',\n",
       "       'optimism', 'sadness', 'neutral'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate(row):\n",
    "    return list(row.values)\n",
    "\n",
    "df['label'] = df.iloc[:, 2:].apply(accumulate, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(reduced_labels, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(text) for text in df['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return ''.join(char for char in text if not char.isdigit())\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    return ''.join(char for char in text if char not in string.punctuation)\n",
    "\n",
    "for i in range(len(df['Text'])):\n",
    "    df['Text'][i] = remove_special_characters(df['Text'][i])\n",
    "    df['Text'][i] = remove_numbers(df['Text'][i])\n",
    "    df['Text'][i] = df['Text'][i].lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,  # change it when optimizing the model\n",
    "        return_token_type_ids=True,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        truncation=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-07 15:46:23.343126: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-07 15:46:23.343479: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-07 15:46:23.435683: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-07 15:46:23.640824: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-07 15:46:25.299163: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/aayushjoshi/ML/MLenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def convert_to_tf_dataset(texts, labels, batch_size=32):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    token_type_ids = []\n",
    "\n",
    "    for text in texts:\n",
    "        tokens = tokenize_text(text)\n",
    "        input_ids.append(tokens['input_ids'])\n",
    "        attention_masks.append(tokens['attention_mask'])\n",
    "        token_type_ids.append(tokens['token_type_ids'])\n",
    "\n",
    "    input_ids = tf.convert_to_tensor(input_ids)\n",
    "    attention_masks = tf.convert_to_tensor(attention_masks)\n",
    "    token_type_ids = tf.convert_to_tensor(token_type_ids)\n",
    "    labels = tf.convert_to_tensor(labels)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': input_ids, 'attention_mask': attention_masks, 'token_type_ids': token_type_ids}, labels))\n",
    "\n",
    "    return dataset.batch(batch_size)\n",
    "\n",
    "texts = list(df['Text'])\n",
    "labels = list(df['label'])\n",
    "batch_size = 8 # another hyperparameter we will fine tune later\n",
    "train_dataset = convert_to_tf_dataset(texts, labels, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "679/679 [==============================] - 4787s 7s/step - loss: 0.6312 - accuracy: 0.3192\n",
      "Epoch 2/10\n",
      "679/679 [==============================] - 4798s 7s/step - loss: 0.5490 - accuracy: 0.3233\n",
      "Epoch 3/10\n",
      "679/679 [==============================] - 4808s 7s/step - loss: 0.5207 - accuracy: 0.3212\n",
      "Epoch 4/10\n",
      "679/679 [==============================] - 4783s 7s/step - loss: 0.5130 - accuracy: 0.3216\n",
      "Epoch 5/10\n",
      "679/679 [==============================] - 4762s 7s/step - loss: 0.5023 - accuracy: 0.3199\n",
      "Epoch 6/10\n",
      "679/679 [==============================] - 4719s 7s/step - loss: 0.4957 - accuracy: 0.3209\n",
      "Epoch 7/10\n",
      "679/679 [==============================] - 4768s 7s/step - loss: 0.4774 - accuracy: 0.3207\n",
      "Epoch 8/10\n",
      "679/679 [==============================] - 4810s 7s/step - loss: 0.4774 - accuracy: 0.3212\n",
      "Epoch 9/10\n",
      "679/679 [==============================] - 4811s 7s/step - loss: 0.4647 - accuracy: 0.3225\n",
      "Epoch 10/10\n",
      "679/679 [==============================] - 4808s 7s/step - loss: 0.4990 - accuracy: 0.3220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f0ca8d4d710>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "model.fit(train_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: berta/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: berta/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('berta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
